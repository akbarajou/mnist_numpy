{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This note contains an **explicit** routine of a neural network for MNIST digit recognizer implemented in numpy from scratch.\n",
    "\n",
    "This is a simple demonstration mainly for pedagogical purposes, which shows the basic workflow of a machine learning algorithm using a simple feedforward neural network. The derivative at the backpropagation stage is computed explicitly through the chain rule.\n",
    "\n",
    "* The model is a 3-layer feedforward neural network (FNN), in which the input layer has 784 units, and the 256-unit hidden layer is activated by ReLU, while the output layer is activated by softmax function to produce a discrete probability distribution for each input. \n",
    "\n",
    "* The loss function, model hypothesis function, and the gradient of the loss function are all implemented from ground-up in numpy in a highly vectorized fashion (no FOR loops).\n",
    "\n",
    "* The training is through a standard gradient descent with step size adaptively changing by Root Mean Square prop (RMSprop), and there is no cross-validation set reserved nor model averaging for simplicity.\n",
    "\n",
    "\n",
    "The code is vectorized and is adapted from the Softmax regression and Neural Network lectures used in [UCI Math 10](https://github.com/scaomath/UCI-Math10). \n",
    "\n",
    "Caveat lector: fluency in linear algebra and multivariate calculus.\n",
    "\n",
    "\n",
    "#### References:\n",
    "* [Stanford Deep Learning tutorial in MATLAB](http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/).\n",
    "* [Learning PyTorch with examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html).\n",
    "* [An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train.csv', 'sample_submission.csv', 'test.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = os.listdir(\"../dataset\")\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Read the data\n",
    "train_data = pd.read_csv('../dataset/train.csv')\n",
    "test_data = pd.read_csv(\"../dataset/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Set up the data\n",
    "y_train = train_data['label'].values\n",
    "X_train = train_data.drop(columns=['label']).values/255\n",
    "X_test = test_data.values/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some examples of the input data\n",
    "We randomly choose 10 samples from the training set and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2,5, figsize=(12,5))\n",
    "axes = axes.flatten()\n",
    "idx = np.random.randint(0,42000,size=10)\n",
    "for i in range(10):\n",
    "    axes[i].imshow(X_train[idx[i],:].reshape(28,28), cmap='gray')\n",
    "    axes[i].axis('off') # hide the axes ticks\n",
    "    axes[i].set_title(str(int(y_train[idx[i]])), color= 'black', fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network structures\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/scaomath/UCI-Math10/master/Lectures/neural_net_3l.png\" alt=\"drawing\" width=\"700\"/>\n",
    "\n",
    "The figure above is a simplication of the neural network used in this example. The circles labeled \"+1\" are the bias units. Layer 1 is the input layer, and Layer 3 is the output layer. The middle layer, Layer 2, is the hidden layer.\n",
    "\n",
    "The neural network in the figure above has 2 input units (not counting the bias unit), 3 hidden units, and 1 output unit. In this actual computation below, the input layer has 784 units, the hidden layer has 256 units, and the output layers has 10 units ($K =10$ classes).\n",
    "\n",
    "The weight matrix $W^{(0)}$ mapping input $\\mathbf{x}$ from the input layer (Layer 1) to the hidden layer (Layer 2) is of shape `(784,256)` together with a `(256,)` bias. Then $\\mathbf{a}$ is the activation from the hidden layer (Layer 2) can be written as:\n",
    "$$\n",
    "\\mathbf{a} = \\mathrm{ReLU}\\big((W^{(0)})^{\\top}\\mathbf{x} + \\mathbf{b}\\big),\n",
    "$$\n",
    "where the ReLU activation function is $\\mathrm{ReLU}(z) = \\max(z,0)$ and can be implemented in a vectorized fashion as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu activation function\n",
    "# THE fastest vectorized implementation for ReLU\n",
    "def relu(x):\n",
    "    x[x<0]=0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax activation, prediction, and the loss function\n",
    "\n",
    "From the hidden layer (Layer 2) to the output layer (layer 3), the weight matrix $W^{(1)}$ is of shape `(256,10)`, the form of which is as follows:\n",
    "$$\n",
    "W^{(1)} =\n",
    "\\begin{pmatrix}\n",
    "| & | & | & | \\\\\n",
    "\\boldsymbol{\\theta}_1 & \\boldsymbol{\\theta}_2 & \\cdots & \\boldsymbol{\\theta}_K \\\\\n",
    "| & | & | & |\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "which maps the activation from Layer 2 to Layer 3 (output layer), and there is no bias because a constant can be freely added to the activation without changing the final output. \n",
    "\n",
    "At the last layer, a softmax activation is used, which can be written as follows combining the weights matrix $W^{(1)}$ that maps the activation $\\mathbf{a}$ from the hidden layer to output layer:\n",
    "$$\n",
    "P\\big(y = k \\;| \\;\\mathbf{a}; W^{(1)}\\big) = \\sigma_k(\\mathbf{a}; W^{(1)}) := \\frac{\\exp\\big(\\boldsymbol{\\theta}^{\\top}_k \\mathbf{a} \\big)}\n",
    "{\\sum_{j=1}^K \\exp\\big(\\boldsymbol{\\theta}^{\\top}_j \\mathbf{a} \\big)}.\n",
    "$$\n",
    "$\\{P\\big(y = k \\;| \\;\\mathbf{a}; W^{(1)}\\big)\\}_{k=1}^K$ is the probability distribution of our model, which estimates the probability of the input $\\mathbf{x}$'s label $y$ is of class $k$. We denote this distribution by a vector \n",
    "$$\\boldsymbol{\\sigma}:= (\\sigma_1,\\dots, \\sigma_K)^{\\top}.$$\n",
    "We hope that this estimate is as close as possible to the true probability: $1_{\\{y=k\\}}$, that is $1$ if the sample $\\mathbf{x}$ is in the $k$-th class and 0 otherwise. \n",
    "\n",
    "Lastly, our prediction $\\hat{y}$ for sample $\\mathbf{x}$ can be made by choosing the class with the highest probability:\n",
    "$$\n",
    "\\hat{y} = \\operatorname{argmax}_{k=1,\\dots,K}  P\\big(y = k \\;| \\;\\mathbf{a}; W^{(1)}\\big). \\tag{$\\ast$}\n",
    "$$\n",
    "\n",
    "Denote the label of the $i$-th input as $y^{(i)}$, and then the sample-wise loss function is the cross entropy measuring the difference of the distribution of this model function above with the true one $1_{\\{y^{(i)}=k\\}}$: denote $W = (W^{(0)}, W^{(1)})$, $b = (\\mathbf{b})$, let $\\mathbf{a}^{(i)}$ be the activation for the $i$-th sample in the hidden layer (Layer 2),\n",
    "$$\n",
    "J_i:= J(W,b;\\mathbf{x}^{(i)},y^{(i)}) := - \\sum_{k=1}^{K} \\left\\{  1_{\\left\\{y^{(i)} = k\\right\\} }\n",
    "\\log P\\big(y^{(i)} = k \\;| \\;\\mathbf{a}^{(i)}; W^{(1)}\\big)\\right\\}. \\tag{1}\n",
    "$$\n",
    "\n",
    "Denote the data sample matrix $X := (\\mathbf{x}^{(1)}, \\dots, \\mathbf{x}^{(N)})^{\\top}$, its label vector as $\\mathbf{y} := (y^{(1)}, \\dots, y^{(N)})$, and then the final loss has an extra $L^2$-regularization term for the weight matrices (not for bias): \n",
    "$$\n",
    "L(W,b; X, \\mathbf{y}) := \\frac{1}{N}\\sum_{i=1}^{N} J_i  + \\frac{\\alpha}{2} \\Big(\\|W^{(0)}\\|^2 + \\|W^{(1)}\\|^2\\Big),\n",
    "\\tag{2}\n",
    "$$\n",
    "where $\\alpha>0$ is a hyper-parameter determining the strength of the regularization, the bigger the $\\alpha$ is, the smaller the magnitudes of the weights will be after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(X,W,b):\n",
    "    '''\n",
    "    Hypothesis function: simple FNN with 1 hidden layer\n",
    "    Layer 1: input\n",
    "    Layer 2: hidden layer, with a size implied by the arguments W[0], b\n",
    "    Layer 3: output layer, with a size implied by the arguments W[1]\n",
    "    '''\n",
    "    # layer 1 = input layer\n",
    "    a1 = X\n",
    "    # layer 1 (input layer) -> layer 2 (hidden layer)\n",
    "    z1 = np.matmul(X, W[0]) + b[0]\n",
    "    \n",
    "    # add one more layer if applicable\n",
    "    \n",
    "    # layer 2 activation\n",
    "    a2 = relu(z1)\n",
    "    # layer 2 (hidden layer) -> layer 3 (output layer)\n",
    "    z2 = np.matmul(a2, W[1])\n",
    "    s = np.exp(z2)\n",
    "    total = np.sum(s, axis=1).reshape(-1,1)\n",
    "    sigma = s/total\n",
    "    # the output is a probability for each sample\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X_in,weights):\n",
    "    '''\n",
    "    Un-used cell for demo\n",
    "    activation function for the last FC layer: softmax function \n",
    "    Output: K probabilities represent an estimate of P(y=k|X_in;weights) for k=1,...,K\n",
    "    the weights has shape (n, K)\n",
    "    n: the number of features X_in has\n",
    "    n = X_in.shape[1]\n",
    "    K: the number of classes\n",
    "    K = 10\n",
    "    '''\n",
    "    \n",
    "    s = np.exp(np.matmul(X_in,weights))\n",
    "    total = np.sum(s, axis=1).reshape(-1,1)\n",
    "    return s / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_pred,y_true):\n",
    "    '''\n",
    "    Loss function: cross entropy with an L^2 regularization\n",
    "    y_true: ground truth, of shape (N, )\n",
    "    y_pred: prediction made by the model, of shape (N, K) \n",
    "    N: number of samples in the batch\n",
    "    K: global variable, number of classes\n",
    "    '''\n",
    "    global K \n",
    "    K = 10\n",
    "    N = len(y_true)\n",
    "    # loss_sample stores the cross entropy for each sample in X\n",
    "    # convert y_true from labels to one-hot-vector encoding\n",
    "    y_true_one_hot_vec = (y_true[:,np.newaxis] == np.arange(K))\n",
    "    loss_sample = (np.log(y_pred) * y_true_one_hot_vec).sum(axis=1)\n",
    "    # loss_sample is a dimension (N,) array\n",
    "    # for the final loss, we need take the average\n",
    "    return -np.mean(loss_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation (Chain rule)\n",
    "\n",
    "The derivative of the cross entropy $J$ in (1), for a single sample and its label $(\\mathbf{x}, y)$ , with respect to the weights and the bias is computed using the following procedure:\n",
    "> **Step 1**: Forward pass: computing the activations $\\mathbf{a} = (a_1,\\dots, a_{n_2})$ from the hidden layer (Layer 2), and $\\boldsymbol{\\sigma} = (\\sigma_1,\\dots, \\sigma_K)$ from the output layer (Layer 3). \n",
    ">\n",
    "> **Step 2**: Derivatives for $W^{(1)}$: recall that $W^{(1)} = (\\boldsymbol{\\theta}_1 ,\\cdots,  \\boldsymbol{\\theta}_K)$ and denote \n",
    "$$\\mathbf{z}^{(2)} = \\big(z^{(2)}_1, \\dots, z^{(2)}_K\\big)  = (W^{(1)})^{\\top}\\mathbf{a} =\n",
    "\\big(\\boldsymbol{\\theta}^{\\top}_1 \\mathbf{a} ,\\cdots,  \\boldsymbol{\\theta}^{\\top}_K \\mathbf{a}\\big),$$ \n",
    "for the $k$-th output unit in the output layer (Layer 3), then\n",
    "$$\n",
    "\\delta^{(2)}_k\n",
    ":= \\frac{\\partial J}{\\partial z_k^{(2)}} = \\Big\\{  P\\big(y = k \\;| \\;\\mathbf{a}; W^{(1)}\\big)- 1_{\\{ y = k\\}} \\Big\\} = \\sigma_k - 1_{\\{ y = k\\}}\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\boldsymbol{\\theta}_k}= \\frac{\\partial J}{\\partial z_k^{(2)}}\\frac{\\partial z_k^{(2)}}{\\partial \\boldsymbol{\\theta}_k} = \\delta^{(2)}_k \\mathbf{a}.\n",
    "$$\n",
    ">\n",
    "> **Step 3**: Derivatives for $W^{(0)}$, $\\mathbf{b}$: recall that $W^{(0)} = (\\boldsymbol{w}_1 ,\\cdots,  \\boldsymbol{w}_{n_2})$, $\\mathbf{b} = (b_1,\\dots, b_{n_2})$, where $n_2$ is the number of units in the hidden layer (Layer 2), and denote \n",
    "$$\\mathbf{z}^{(1)} = (z_1^{(1)}, \\dots, z_{n_2}^{(1)})  = (W^{(0)})^{\\top}\\mathbf{x} + \\mathbf{b} =\n",
    "\\big(\\mathbf{w}^{\\top}_1 \\mathbf{x} +b_1 ,\\cdots,  \\mathbf{w}^{\\top}_{n_2} \\mathbf{x} + b_{n_2}\\big),$$ \n",
    "for each node $i$ in the hidden layer (Layer $2$), $i=1,\\dots, n_2$, then\n",
    "$$\\delta^{(1)}_i : = \\frac{\\partial J}{\\partial z^{(1)}_i}  =\n",
    "\\frac{\\partial J}{\\partial a_i} \n",
    "\\frac{\\partial a_i}{\\partial z^{(1)}_i}=\n",
    "\\frac{\\partial J}{\\partial \\mathbf{z}^{(2)}}\n",
    "\\cdot\\left(\\frac{\\partial \\mathbf{z}^{(2)}}{\\partial a_i} \n",
    "\\frac{\\partial a_i}{\\partial z^{(1)}_i}\\right)\n",
    "\\\\\n",
    "=\\left( \\sum_{k=1}^{K} \\frac{\\partial J}{\\partial {z}^{(2)}_k}\n",
    "\\frac{\\partial {z}^{(2)}_k}{\\partial a_i}  \\right) f'(z^{(1)}_i) = \\left( \\sum_{k=1}^{K} w_{ki} \\delta^{(2)}_k \\right) 1_{\\{z^{(1)}_i\\; > 0\\}},\n",
    "$$\n",
    "where $1_{\\{z^{(1)}_i\\; > 0\\}}$ is ReLU activation $f$'s (weak) derivative, and the partial derivative of the $k$-th component (before activated by the softmax) in the output layer ${z}^{(2)}_k$ with respect to the $i$-th activation $a_i$ from the hidden layer is the weight $w^{(1)}_{ki}$. Thus\n",
    ">\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_{ji}}  = x_j \\delta_i^{(1)} ,\\;\n",
    "\\frac{\\partial J}{\\partial b_{i}} = \\delta_i^{(1)}, \\;\\text{ and }\\;\n",
    "\\frac{\\partial J}{\\partial \\mathbf{w}_{i}}  = \\delta_i^{(1)}\\mathbf{x} ,\\;\n",
    "\\frac{\\partial J}{\\partial \\mathbf{b}} = \\boldsymbol{\\delta}^{(1)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(W,b,X,y,alpha=1e-4):\n",
    "    '''\n",
    "    Step 1: explicit forward pass h(X;W,b)\n",
    "    Step 2: backpropagation for dW and db\n",
    "    '''\n",
    "    K = 10\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    ### Step 1:\n",
    "    # layer 1 = input layer\n",
    "    a1 = X\n",
    "    # layer 1 (input layer) -> layer 2 (hidden layer)\n",
    "    z1 = np.matmul(X, W[0]) + b[0]\n",
    "    # layer 2 activation\n",
    "    a2 = relu(z1)\n",
    "    \n",
    "    # one more layer\n",
    "    \n",
    "    # layer 2 (hidden layer) -> layer 3 (output layer)\n",
    "    z2 = np.matmul(a2, W[1])\n",
    "    s = np.exp(z2)\n",
    "    total = np.sum(s, axis=1).reshape(-1,1)\n",
    "    sigma = s/total\n",
    "    \n",
    "    ### Step 2:\n",
    "    \n",
    "    # layer 2->layer 3 weights' derivative\n",
    "    # delta2 is \\partial L/partial z2, of shape (N,K)\n",
    "    y_one_hot_vec = (y[:,np.newaxis] == np.arange(K))\n",
    "    delta2 = (sigma - y_one_hot_vec)\n",
    "    grad_W1 = np.matmul(a2.T, delta2)\n",
    "    \n",
    "    # layer 1->layer 2 weights' derivative\n",
    "    # delta1 is \\partial a2/partial z1\n",
    "    # layer 2 activation's (weak) derivative is 1*(z1>0)\n",
    "    delta1 = np.matmul(delta2, W[1].T)*(z1>0)\n",
    "    grad_W0 = np.matmul(X.T, delta1)\n",
    "    \n",
    "    # Possible student project: extra layer of derivative\n",
    "    \n",
    "    # no derivative for layer 1\n",
    "    \n",
    "    # the alpha part is the derivative for the regularization\n",
    "    # regularization = 0.5*alpha*(np.sum(W[1]**2) + np.sum(W[0]**2))\n",
    "    \n",
    "    \n",
    "    dW = [grad_W0/N + alpha*W[0], grad_W1/N + alpha*W[1]]\n",
    "    db = [np.mean(delta1, axis=0)]\n",
    "    # dW[0] is W[0]'s derivative, and dW[1] is W[1]'s derivative; similar for db\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters and network initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 5e-1\n",
    "alpha = 1e-6 # regularization\n",
    "gamma = 0.99 # RMSprop\n",
    "eps = 1e-3 # RMSprop\n",
    "num_iter = 1000 # number of iterations of gradient descent\n",
    "n_H = 256 # number of neurons in the hidden layer\n",
    "n = X_train.shape[1] # number of pixels in an image\n",
    "K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "np.random.seed(1127825)\n",
    "W = [1e-1*np.random.randn(n, n_H), 1e-1*np.random.randn(n_H, K)]\n",
    "b = [np.random.randn(n_H)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent: training of the network\n",
    "\n",
    "In the training, we use a GD-variant of the RMSprop: for $\\mathbf{w}$ which stands for the parameter vector in our model\n",
    "> Choose $\\mathbf{w}_0$, $\\eta$, $\\gamma$, $\\epsilon$, and let $g_{-1} = 1$ <br><br>\n",
    ">    For $k=0,1,2, \\cdots, M$<br><br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp;  $g_{k} = \\gamma g_{k-1} + (1 - \\gamma)\\, \\left|\\partial_{\\mathbf{w}} L (\\mathbf{w}_k)\\right|^2$<br><br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp;    $\\displaystyle\\mathbf{w}_{k+1} =  \\mathbf{w}_k -  \\frac{\\eta} {\\sqrt{g_{k}+ \\epsilon}} \\partial_{\\mathbf{w}} L(\\mathbf{w}_k)$  \n",
    "\n",
    "### Remark: \n",
    "The training takes a while since we use the gradient descent for all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_kg_hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss after 1 iterations is 7.6743264\n",
      "Training accuracy after 1 iterations is 24.9262%\n",
      "gW0=1.0839 gW1=1.3825 gb0=0.9923\n",
      "etaW0=0.4800 etaW1=0.4251 etab0=0.5017\n",
      "|dW0|=3.06418 |dW1|=6.26530 |db0|=0.48001 \n",
      "\n",
      "Cross-entropy loss after 501 iterations is 0.13586248\n",
      "Training accuracy after 501 iterations is 96.1429%\n",
      "gW0=0.3495 gW1=0.1121 gb0=0.0153\n",
      "etaW0=0.8446 etaW1=1.4870 etab0=3.9189\n",
      "|dW0|=0.05204 |dW1|=0.02360 |db0|=0.00697 \n",
      "\n",
      "Final cross-entropy loss is 0.06835161\n",
      "Final training accuracy is 97.9786%\n",
      "CPU times: user 1h 11min 2s, sys: 4min 55s, total: 1h 15min 58s\n",
      "Wall time: 32min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gW0 = gW1 = gb0 = 1\n",
    "\n",
    "for i in range(num_iter):\n",
    "    dW, db = backprop(W,b,X_train,y_train,alpha)\n",
    "    \n",
    "    gW0 = gamma*gW0 + (1-gamma)*np.sum(dW[0]**2)\n",
    "    etaW0 = eta/np.sqrt(gW0 + eps)\n",
    "    W[0] -= etaW0 * dW[0]\n",
    "    \n",
    "    gW1 = gamma*gW1 + (1-gamma)*np.sum(dW[1]**2)\n",
    "    etaW1 = eta/np.sqrt(gW1 + eps)\n",
    "    W[1] -= etaW1 * dW[1]\n",
    "    \n",
    "    gb0 = gamma*gb0 + (1-gamma)*np.sum(db[0]**2)\n",
    "    etab0 = eta/np.sqrt(gb0 + eps)\n",
    "    b[0] -= etab0 * db[0]\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        # sanity check 1\n",
    "        y_pred = h(X_train,W,b)\n",
    "        print(\"Cross-entropy loss after\", i+1, \"iterations is {:.8}\".format(\n",
    "              loss(y_pred,y_train)))\n",
    "        print(\"Training accuracy after\", i+1, \"iterations is {:.4%}\".format( \n",
    "              np.mean(np.argmax(y_pred, axis=1)== y_train)))\n",
    "        \n",
    "        # sanity check 2\n",
    "        print(\"gW0={:.4f} gW1={:.4f} gb0={:.4f}\\netaW0={:.4f} etaW1={:.4f} etab0={:.4f}\"\n",
    "              .format(gW0, gW1, gb0, etaW0, etaW1, etab0))\n",
    "        \n",
    "        # sanity check 3\n",
    "        print(\"|dW0|={:.5f} |dW1|={:.5f} |db0|={:.5f}\"\n",
    "             .format(np.linalg.norm(dW[0]), np.linalg.norm(dW[1]), np.linalg.norm(db[0])), \"\\n\")\n",
    "        \n",
    "        # reset RMSprop\n",
    "        gW0 = gW1 = gb0 = 1\n",
    "\n",
    "y_pred_final = h(X_train,W,b)\n",
    "print(\"Final cross-entropy loss is {:.8}\".format(loss(y_pred_final,y_train)))\n",
    "print(\"Final training accuracy is {:.4%}\".format(np.mean(np.argmax(y_pred_final, axis=1)== y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions for testing data\n",
    "The prediction labels are generated by $(\\ast)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "y_pred_test = np.argmax(h(X_test,W,b), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating submission using pandas for grading\n",
    "submission = pd.DataFrame({'ImageId': range(1,len(X_test)+1) ,'Label': y_pred_test })\n",
    "submission.to_csv(\"simplemnist_result.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
